\documentclass{book}

%Post to: https://stats.stackexchange.com/questions/21346/reference-book-for-linear-algebra-applied-to-statistics

\usepackage[top=1in, bottom=1.25in, left=1.25in, right=1.25in]{geometry}

\usepackage{amsfonts, amsmath}
\usepackage{commath}
\usepackage[yyyymmdd,hhmmss]{datetime}
\usepackage{graphbox}
\usepackage[hidelinks]{hyperref}
\usepackage{marginnote}
\usepackage{mathtools}
\usepackage{parskip}
\usepackage{titlesec}
\usepackage{xcolor}
\usepackage{optidef}

\usepackage{cellspace}%
\setlength\cellspacetoplimit{3pt}
\setlength\cellspacebottomlimit{3pt}

\usepackage{makeidx}
\makeindex

\usepackage[numbers,sort&compress]{natbib}
\bibliographystyle{unsrtnat}

%Make equations be numbered continuously through book
\usepackage{chngcntr}
\counterwithout{equation}{chapter}

\renewcommand{\sectionautorefname}{\textsection}
\renewcommand{\subsectionautorefname}{\textsection}
\renewcommand{\subsubsectionautorefname}{\textsection}

\input{z_math_commands}

\hypersetup{
  pdfauthor={Richard Barnes (ORCID: 0000-0002-0204-6040)},%
  pdftitle={Matrix Forensics},%
%            pdfsubject={Whatever},%
  pdfkeywords = {matrix algebra, matrix relations, matrix identities, linear algebra},%
  pdfproducer = {LaTeX},%
  pdfcreator  = {pdfLaTeX}
}


\usepackage{fancyhdr}
% \renewcommand{\chaptermark}[1]{\markboth{#1}{#1}}
\setlength{\headheight}{15.2pt}
\pagestyle{fancy}

\lhead[\thepage]{\leftmark}
% \chead[]{<odd output>}
\rhead[\leftmark]{\thepage}

\renewcommand{\footrulewidth}{0.4pt}% default is 0pt
\lfoot[\footnotesize{Richard Barnes. Matrix Forensics. \today-\currenttime. \href{https://github.com/r-barnes/MatrixForensics}{github.com/r-barnes/MatrixForensics}}. \input{/tmp/matrix_forensics_version.info}\!\!.]{\footnotesize{Richard Barnes. Matrix Forensics. \today-\currenttime. \href{https://github.com/r-barnes/MatrixForensics}{github.com/r-barnes/MatrixForensics}}. \input{/tmp/matrix_forensics_version.info}\!\!.} %  [<even output>]{<odd output>}
\cfoot[]{}
\rfoot[]{}


\newcommand{\eqcite}[1]{\marginnote{\citep{#1}}}

%Adjust chapter formatting
\newcommand{\hsp}{\hspace{20pt}}
\definecolor{gray75}{gray}{0.75}
\titleformat{\chapter}[hang]{\Huge\bfseries}{\thechapter\hsp\textcolor{gray75}{$|$}\hsp}{0pt}{\Huge\bfseries}
\titlespacing*{\chapter}{0pt}{0pt}{20pt} %? BEFORE AFTER

%Ensure chapters start on the same page
\usepackage{etoolbox}
\makeatletter
\patchcmd{\chapter}{\if@openright\cleardoublepage\else\clearpage\fi}{\clearpage}{}{}
\makeatother


\begin{document}

\input{title}


\tableofcontents

\input{introduction}

\input{nomenclature}

\input{basics}

\input{derivatives}

\input{rogue_gallery}

\input{decompositions}

\input{eigenvalues}

\input{norms}



\chapter{Bounds} %TODO: Reorganize

\section{Matrix Gain}
\begin{equation}
\lambda_\textrm{min}(\mA^T\mA)\le \frac{\norm{\mA\vx}_2^2}{\norm{\vx}_2^2}\le\lambda_\textrm{max}(\mA^T\mA)
\end{equation}

\begin{equation}
\max_{\vx\ne0} \frac{\norm{\mA\vx}_2}{\norm{\vx}_2}=\norm{\mA}_2=\sqrt{\lambda_\textrm{max}(\mA^T\mA)}\implies\vx=u_1
\end{equation}

\begin{equation}
\min_{\vx\ne0} \frac{\norm{\mA\vx}_2}{\norm{\vx}_2}=\sqrt{\lambda_\textrm{min}(\mA^T\mA)}\implies\vx=u_n
\end{equation}

\section{Rayleigh quotients}
The Rayleigh quotient of $\mA\in\sSn$ is given by
\begin{equation}
\frac{\vx^T \mA \vx}{\vx^T\vx}~~\vx\ne0
\end{equation}

\begin{equation}
\lambda_\textrm{min}(\mA)\le \frac{\vx^T \mA \vx}{\vx^T\vx} \le \lambda_\textrm{max}(\mA)~~\vx\ne0
\end{equation}

\begin{align}
\lambda_\textrm{max}(A)&=\max_{\vx: \norm{\vx}_2=1} \vx^T\mA\vx=u_1 \\
\lambda_\textrm{min}(A)&=\min_{\vx: \norm{\vx}_2=1} \vx^T\mA\vx=u_n
\end{align}
where $u_1$ and $u_n$ are the eigenvectors associated with $\lambda_\textrm{max}$ and $\lambda_\textrm{min}$, respectively.







\chapter{Equations}

\section{Linear Equations}
The linear equation $\mA\vx=\vy$ with $\mA\in\sRmn$ admits a solution iff $\rank([\mA \vy])=\rank(\mA)$. If this is satisfied, the set of all solutions is an affine set $\mathcal{S}=\{\vx=\bar \vx+z: z\in\ns(\mA)\}$ where $\bar \vx$ is any vector such that $\mA\bar\vx=\vy$. The solution is unique if $\ns(\mA)=\{0\}$.

$\mA\vx=\vy$ is \textit{overdetermined} if it is tall/skinny ($m>n$); that is, if there are more equations than unknowns. If $\rank(\mA)=n$ then $\dim\ns(\mA)=0$, so there is either no solution or one solution. Overdetermined systems often have no solution ($\vy\notin\range(\mA)$), so an approximate solution is necessary. See \autoref{sec:least-squares}.

$\mA\vx=\vy$ is \textit{underdetermined} if it is short/wide ($n>m$); that is, if has more unknowns than equations. If $\rank(\mA)=m$ then $\range(\mA)=\sRm$, so $\dim\ns(\mA)=n-m>0$, so the set of solutions is infinite. Therefore, finding a single solution that optimizes some quantity is of interest.

$\mA\vx=\vy$ is \textit{square} if $n=m$. If $\mA$ is invertible, then the equations have the unique solution $\vx=\mA^{-1}\vy$. See \autoref{sec:minimum-norm}.

\section{Least-Squares}
\label{sec:least-squares}
For an overdetermined system we wish to find:
\begin{equation}
\min_\vx \norm{\mA\vx-\vy}_2^2
\end{equation}
Since $\mA\vx\in\range(\mA)$, we need a point $\tilde \vy = \mA\vx^*\in\range(\mA)$ closest to $\vy$. This point lies in the nullspace of $\mA^T$, so we have $\mA^T(\vy-\mA\vx^*)=0$. There is always a solution to this problem and, if $\rank(\mA)=n$, it is unique~\citep[p.\ 161]{Calafiore2014}
\begin{equation}
\vx^*=(\mA^T\mA)^{-1}\mA^T\vy
\end{equation} %TODO: Check

\subsection{Regularized least-squares with low-rank data}

For $\mA\in\sRmn$, $\vy\in\sRm$, $\lambda\ge0$, the regularized least-squares problem
\begin{equation}
\textrm{argmin}_\vx \norm{\mA\vx-\vy}_2^2 + \lambda\norm{\vx}_2^2
\end{equation}
has a closed form solution
\begin{equation}
\label{equ:regularized_least_squares}
\vx = (\mA^T\mA   + \lambda \mI)^{-1}\mA^T\vy
\end{equation}
However, if $\mA$ has a $\rank{r}\ll\min(n,m)$ and a known low-rank decomposition $\mA=\mL\mR^T$ with $\mL\in\mathbb{R}^{m,r}$ and $\mR\in\mathbb{R}^{n,r}$, then we can rewrite \autoref{equ:regularized_least_squares} as
\begin{equation}
\vx = (\mR^T \mR\mL^T \mL   + \lambda \mI)^{-1}\mL^T\vy
\end{equation}
This decreases the time complexity from $O(mn^2 + n^\omega)$ to $O(nr^2+mr^2)$.

\section{Minimum Norm Solutions}
\label{sec:minimum-norm}
For undertermined systems in which $\mA\in\sRmn$ with $m<n$. We wish to find
\begin{equation}
\min_{\vx: \mA\vx=\vy} \norm{\vx}_2
\end{equation}
The solution $\vx^*$ must be orthogonal to $\ns(\mA)$, so $\vx^*\in\range(\mA^T)$, so $\vx^*=\mA^Tc$ for some $c$. Substituting into $\mA\vx=\vy$ gives $\mA \mA^T c=\vy$, therefore~\citep[p.\ 162]{Calafiore2014}:
\begin{equation}
\vx^*=\mA^T(\mA\mA^T)^{-1}\vy
\end{equation}

\section{The Sylvester Equation: $\mA\mX+\mX^T\mB=\mC$}
The equation
\begin{equation}
\mA\mX+\mX^T\mB=\mC
\end{equation}
is called a T-Sylvester equation, or *-Sylvester equation in the complex case. It can be solved using methods from, e.g.:~\citet{Teran2011,Teran2019,Dopico2016}.


\input{updates}

\input{optimization}

\input{algorithmics}


\bibliography{refs}

\printindex

\end{document}

TODO:
Orthogonal matrix = all eigenvalues are 1 or -1

Centering matrix
Distance matrix



For two vectors $b$ and $x$, $p=\frac{b^Tx}{b^Tb}b$ is the projection of $x$ onto $b$.

TODO: Gilber Strang (2016, p563: Matrix Factorizations)

TODO: Strang 2016, p. 583, List of symbols and computer codes


TODO: Add Gram-Schmidt procedure
TODO: Add computational efficiency notes for QR


Highlighting matrix example: http://www.texample.net/tikz/examples/highlighting-matrix/