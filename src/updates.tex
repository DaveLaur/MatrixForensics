\chapter{Updates}

%TODO
% \section{Low-Rank Updates to $Ax=b$}

% Given nonsingular $\mA\in\sRnn$ and $\vu,\vv\in\sRn$ with $1+\vv^T\mA^{-1}\vu\ne0$, if we have solved $\mA\vx=\vb$, then we can quickly find $(\mA+\vu\vv^T)\bar \vx = \vb$. Namely,

\section{Woodbury Identity (rank-$k$ update to inverse)}
\label{sec:update:woodbury}

%TODO: Add material from "ON DERIVING THE INVERSE OF A SUM OF MATRICES"
%TODO: Add material from "Generalization of the matrix inversion lemma"

The inverse of a rank-$k$ update of some matrix $\mA$ can be computed by doing a rank-$k$ update of $\mAi$.

\begin{align}
%(\mA + \mC\mB\mC^T)^{-1} = \mAi - \mAi\mC(\mBi + \mC^T\mAi\mC)^{-1}\mC^T\mAi %TODO: What is this good for?
(\mA+\mU\mC\mV)^{-1} &= \mAi - \mAi\mU(\mBi+\mV\mAi\mU)^{-1}\mV\mAi \\
\end{align} %TODO
where $\mA\in\sRnn$, $\mC\in\sRkk$, $\mU\in\sRnk$, $\mV\in\sRkn$, and $\mA$ and $\mC$ non-singular.

If $\mU$ and $\mV$ are vectors, then the Woodbury Identity reduces to the Sherman--Morrison formula (\autoref{sec:update:sherman}).

If $\mP,\mR$ are positive definite and $\mP\in\sRnn$, $\mR\in\sRkk$, and $\mB\in\sRkn$, then
\begin{align}
(\mPi + \mB^T\mRi\mB)^{-1} &= \mP - \mP\mB^T(\mB\mP\mB^T+\mR)^{-1}\mB\mP  \eqcite{WellingXXXX} \\
(\mPi + \mB^T\mRi\mB)^{-1} \mB^T\mRi &= \mP\mB^T(\mB\mP\mB^T+\mR)^{-1}    \eqcite{WellingXXXX}
\end{align}

\section{Sherman--Morrison Formula (rank-1 update to inverse)}
\label{sec:update:sherman}
The inverse of a rank-1 update of some matrix $\mA$ can be computed by doing a rank-1 update of $\mAi$.
\begin{equation}
(\mA + \vu\vv^T)^{-1}=\mAi-\frac{\mAi\vu\vv^T\mAi}{1+\vv^T\mAi\vu}
\end{equation}
This is a special case of the Woodbury Identity (\autoref{sec:update:woodbury}).


\section{Removing a row from $\mA^T\mA$ ($\mA^T\mA\rightarrow \mA_{\bs i}^T\mA_{\bs i}$)}

\textbf{Plain English:} Matrix times its transpose after eliminating row $i$ from the matrix

\textbf{Inputs:} $\mA\in\sR^{k,m},\vu\in\sRm,\vv\in\sRn$ and $i$, the row to remove from $\mA$

\textbf{Reduces to:} $\mC\in\sR^{k,l}$

\textbf{Algorithm:}

% Recall that
% \begin{equation}
% (\mA\mB)_{kl} = \sum_m \mA_{km}\mB_{ml}~~~\mA\in\sR^{k,m},\mB\in\sR^{m,l}
% \end{equation}
% then we have that
% \begin{equation}
% (\mA^T\mA)_{kl} = \sum_{m} \mA_{mk}\mA_{ml}=\sum_{m\ne i} \mA_{mk}\mA_{ml} + \mA_{jk}\mA_{jl}=\sum_{m\ne i} \mA_{mk}\mA_{ml} + (\mA_{k*})_{j} (\mA_{l*})_{j}
% \end{equation}
% where $(\mA_k*)_{j}$ is the $j$th element of the $k$th column of $\mA$. %TODO

% Thus,
\begin{align}
\mA_{\bs i}^T\mA_{\bs i} &= \mA^T\mA-\mA_{*i}\mA_{*i}^T \\
\noalign{Similarly:}
\mA_{\bs i}^T  y_{\bs i} &= \mA^Ty  -\mA_{*i}y_i^T
\end{align}


\section{$\mathbf{1}_r^T \mA \mathbf{1}_c$}

\textbf{Plain English:} The sum of the elements of the matrix.

\textbf{Reduces to:} Scalar

\textbf{Notation:} For $\mA \in \mathbb{R}^{r\times c}$, $\mathbf{1}_r$ is in $\mathbb{R}^{r \times 1}$ and $\mathbf{1}_c$ is in $\mathbb{R}^{c \times 1}$.

\textbf{Algorithm:} Traverse all the element of the matrix in order keeping track of the sum. For applications where accuracy is important and the matrices have a large dynamic range, Kahan summation or a similar technique should be used.

\textbf{Update Algorithm:} If an entry changes, subtract its old value from the sum and add its new value to the sum.

\section{$\mathbf{e}_i \mA \mathbf{e}_j$}

\textbf{Plain English:} Extract element $\mA_{ij}$ from the matrix

\textbf{Reduces to:} Scalar

\textbf{Notation:} TODO

\textbf{Algorithm:} TODO

\textbf{Update Algorithm:} TODO


\section{$\vx^T \mA \vx$}

\textbf{Plain English:} TODO

\textbf{Reduces to}: Scalar

\textbf{Notation:} $\mA$ must be in $\mathbb{R}^{i\times i}$. $\vx$ is in $\mathbb{R}^{i \times 1}$.

\textbf{Algorithm:} TODO

\textbf{Update Algorithm:} We make use of the identity $(\vx^T \mA \vx)=\sum_{i,j}\big((\vx \vx^T)\circ\mA\big)$. If an entry $\mA_{i,j}$ in the matrix changes subtract its old value $\vx_i \vx_j \mA_{ij}$ and add the new value $\vx_i \vx_j \mA_{ij}'$. If an entry $\vx_i$ changes TODO.
