\documentclass{book}

%Post to: https://stats.stackexchange.com/questions/21346/reference-book-for-linear-algebra-applied-to-statistics


\usepackage{amsfonts, amsmath}
\usepackage{commath}
\usepackage[yyyymmdd,hhmmss]{datetime}
\usepackage[hidelinks]{hyperref}
\usepackage{marginnote}
\usepackage{mathtools}
\usepackage{parskip}
\usepackage{titlesec}
\usepackage{xcolor}

\usepackage[numbers,sort&compress]{natbib}
\bibliographystyle{unsrtnat}

%Make equations be numbered continuously through book
\usepackage{chngcntr}
\counterwithout{equation}{chapter}

\newcommand{\mA}{\mathbf{A}}
\newcommand{\mB}{\mathbf{B}}
\newcommand{\mC}{\mathbf{C}}
\newcommand{\mD}{\mathbf{D}}
\newcommand{\mH}{\mathbf{H}}
\newcommand{\mI}{\mathbf{I}}
\newcommand{\mL}{\mathbf{L}}
\newcommand{\mM}{\mathbf{M}}
\newcommand{\mP}{\mathbf{P}}
\newcommand{\mQ}{\mathbf{Q}}
\newcommand{\mR}{\mathbf{R}}
\newcommand{\mU}{\mathbf{U}}
\newcommand{\mV}{\mathbf{V}}
\newcommand{\mX}{\mathbf{X}}
\newcommand{\mY}{\mathbf{Y}}
\newcommand{\va}{\mathbf{a}}
\newcommand{\vb}{\mathbf{b}}
\newcommand{\vp}{\mathbf{p}}
\newcommand{\vq}{\mathbf{q}}
\newcommand{\vu}{\mathbf{u}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\vz}{\mathbf{z}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\eig}{eig}
\DeclareMathOperator{\trace}{tr}
\DeclareMathOperator{\rank}{rank}
\newcommand{\sPSD}{\mathbb{S}^n_+}
\newcommand{\sC}{\mathbb{C}}
\newcommand{\sCmn}{\mathbb{C}^{m,n}}
\newcommand{\sCnn}{\mathbb{C}^{n,n}}
\newcommand{\sR}{\mathbb{R}}
\newcommand{\sRm}{\mathbb{R}^{m}}
\newcommand{\sRn}{\mathbb{R}^{n}}
\newcommand{\sRnm}{\mathbb{R}^{n,m}}
\newcommand{\sRmn}{\mathbb{R}^{m,n}}
\newcommand{\sRnn}{\mathbb{R}^{n,n}}
\newcommand{\sRmm}{\mathbb{R}^{m,m}}
\newcommand{\sSn}{\mathbb{S}^{n}}
\newcommand{\ispsd}{\succeq}
\newcommand{\ispd}{\succ}
\newcommand{\pinv}{\!^+}
\newcommand{\ns}{\mathcal{N}}
\newcommand{\range}{\mathcal{R}}

\hypersetup{
  pdfauthor={Richard Barnes (ORCID: 0000-0002-0204-6040)},%
  pdftitle={Matrix Forensics},%
%            pdfsubject={Whatever},%
  pdfkeywords = {matrix algebra, matrix relations, matrix identities, linear algebra},%
  pdfproducer = {LaTeX},%
  pdfcreator  = {pdfLaTeX}
}

\newcommand{\eqcite}[1]{\marginnote{\citep{#1}}}

%Adjust chapter formatting
\newcommand{\hsp}{\hspace{20pt}}
\definecolor{gray75}{gray}{0.75}
\titleformat{\chapter}[hang]{\Huge\bfseries}{\thechapter\hsp\textcolor{gray75}{$|$}\hsp}{0pt}{\Huge\bfseries}
\titlespacing*{\chapter}{0pt}{0pt}{20pt} %? BEFORE AFTER

%Ensure chapters start on the same page
\usepackage{etoolbox}
\makeatletter
\patchcmd{\chapter}{\if@openright\cleardoublepage\else\clearpage\fi}{\clearpage}{}{}
\makeatother


\begin{document}

\input{title}


\tableofcontents

\input{introduction}

\input{nomenclature}

\input{derivatives}

\input{rogue_gallery}

\input{decompositions}




\chapter{Matrix Properties}

\begin{equation}
\mA(\mB+\mC)=\mA\mB+\mA\mC~~\textrm{(left distributivity)}
\end{equation}

\begin{equation}
(\mB+\mC)\mA=\mB\mA+\mC\mA~~\textrm{(right distributivity)}
\end{equation}

\begin{equation}
\mA\mB\ne\mB\mA~~(\textrm{in general})
\end{equation}

\begin{equation}
(\mA\mB)\mC=\mA(\mB\mC)~~(\textrm{associativity})
\end{equation}

\chapter{Transpose Properties}

\begin{equation}
(\mA\mB)^T=\mB^T\mA^T
\end{equation}

\begin{equation}
(\mA+\mB)^T=\mA^T+\mB^T
\end{equation}

\begin{equation}
(\mA^{-1})^T=(\mA^T)^{-1}
\end{equation}


\chapter{Determinant Properties}

Geometrically, if a unit volume is acted on by $\mA$, then $|\det(\mA)|$ indicates the volume after the transformation.

\begin{align}
\det(I_n)     &= 1                                  \\
\det(\mA^T)   &= \det(\mA)                          \\
\det(\mA^{-1})&= \frac{1}{\det(\mA)}=\det(\mA)^{-1} \\
\det(AB)      &= \det(BA)                           \\
\det(AB)      &= \det(A)\det(B)~~\mA,\mB\in\sRnn    \\
\det(c\mA)    &= c^n\det(\mA)~~\mA\in\sRnn          \\
\det(\mA)     &= \prod \eig(\mA)                    
\end{align}


\chapter{Trace Properties}
For $\mA\in\sRnn$
\begin{equation}
\trace(\mA)=\sum_{i=1}^n \mA_{ii}
\end{equation}

\begin{align}
\trace(\mA+\mB)&=\trace(\mA)+\trace(\mB) \\
\trace(c\mA)   &=c\trace(\mA)            \\
\trace(\mA)    &=\trace(\mA^T)
\end{align}

For $\mA,\mB,\mC,\mD$ of compatible dimensions,

\begin{equation}
\trace(\mA^T\mB)=\trace(\mA\mB^T)=\trace(\mB^T\mA)=\trace(\mB\mA^T)
\end{equation}

\begin{equation}
\trace(\mA\mB\mC\mD)=\trace(\mB\mC\mD\mA)=\trace(\mC\mD\mA\mB)=\trace(\mD\mA\mB\mC)
\end{equation}
(Invariant under cyclic permutations)



\chapter{Inverse Properties}
The inverse of $\mA\in\sCnn$ is denoted $\mA^{-1}$ and defined such that
\begin{equation}
\mA\mA^{-1}=\mA^{-1}\mA=\mI_n
\end{equation}
where $\mI_n$ is the $n \times n$ identity matrix. $\mA$ is nonsingular if $\mA^{-1}$ exists; otherwise, $\mA$ is singular.


If individual inverses exist
\begin{equation}
(\mA\mB)^{-1}=\mB^{-1}\mA^{-1}
\end{equation}
more generally
\begin{equation}
(\mA_1\mA_2\ldots\mA_n)^{-1}=\mA_n^{-1}\ldots\mA_2^{-1}\mA_1^{-1}
\end{equation}

\begin{equation}
(\mA^{-1})^T=(\mA^T)^{-1}
\end{equation}




\chapter{Pseudo-Inverse Properties}
For $\mA\in\sRmn$, a pseudoinverse satisfies:
\begin{align}
\mA\mA\pinv\mA      &= \mA         \\
\mA\pinv\mA\mA\pinv &= \mA\pinv    \\
(\mA\mA\pinv)^T     &= \mA\mA\pinv \\
(\mA\pinv\mA)^T     &= \mA\pinv\mA
\end{align}

\section{Moore-Penrose Pseudoinverse}
\begin{equation}
\mA\pinv = \mV \mD^{-1} \mU^T
\end{equation}
where the foregoing comes from a singular-value decomposition and $\mD^{-1}=\diag(\frac{1}{\sigma_1},\ldots,\frac{1}{\sigma_r})$

\subsection*{Special Properties}
\begin{itemize}
\item $\mA\pinv=\mA^{-1}$ if $\mA\in\sRnn$ and $\mA$ is square and nonsingular.
\item $\mA\pinv=(\mA^T\mA)^{-1}\mA^T$, if $\mA\in\sRmn$ is full column rank ($r=n\le m$). $\mA\pinv$ is a left inverse of $\mA$, so $\mA\pinv\mA=\mV_r\mV_r^T=\mV\mV^T=\mI_n$.
\item $\mA\pinv=\mA^T(\mA\mA^T)^{-1}$, if $\mA\in\sRmn$ is full row rank ($r=m\le n$). $\mA\pinv$ is a right inverse of $\mA$, so $\mA\mA\pinv=\mU_r\mU_r^T=\mU\mU^T=\mI_m$.
\end{itemize} %TODO: Check these



\chapter{Hadamard Identities}

\begin{align}
(\mA\circ\mB)_{ij}    &= A_{ij}B_{ij}~\forall~i,j                            \\
\mA\circ\mB           &= \mB\circ\mA                    \eqcite{million2007} \\
\mA\circ(\mB\circ\mC) &= (\mA\circ\mB)\circ\mC                               \\
\mA\circ(\mB+\mC)     &= \mA\circ\mB+\mA\circ\mC        \eqcite{million2007} \\
a(\mA\circ\mB)        &= (a\mA)\circ\mB =\mA\circ(a\mB) \eqcite{million2007} \\
(\mA^T\circ\mB^T)     &= (\mA\circ\mB)^T                                     \\
(\mA^T\circ\mB^T)     &= (\mA\circ\mB)^T                                     \\
(x^T \mA x)           &= \sum_{i,j}\big((x x^T)\circ\mA\big)
\end{align}


\chapter{Eigenvalue Properties}

$\lambda\in\mathbb{C}$ is an eigenvalue of $\mA\in\sRnn$ and $u\in\mathbb{C}^n$ is a corresponding eigenvector if $\mA\vu=\lambda\vu$ and $\vu\ne0$. Equivalantly, $(\lambda \mI_n-\mA)\vu=0$ and $\vu\ne0$. Eigenvalues satisfy the equation $\det(\lambda\mI_n-\mA)=0$.

Any matrix $\mA\in\sRnn$ has $n$ eigenvalues, though some may be repeated. $\lambda_1$ is the largest eigenvalue and $\lambda_n$ the smallest.

\begin{equation}
\eig(\mA\mA^T)=\eig(\mA^T\mA)
\end{equation}
(Note that the number of entries in $\mA\mA^T$ and $\mA^T\mA$ may differ significantly leading to different compute times.)

\begin{equation}
\eig(\mA^T\mA)\ge0
\end{equation}

\section*{Computation}

TODO: eigsh, small eigen value extraction, top-k







\chapter{Norms}

\section{Matrices}
Matrix norms satisfy some properties:
\begin{align}
f(\mA)    &\ge 0             \\
f(\mA)    &=   0  \iff \mA=0 \\
f(c\mA)   &=   |c|f(\mA)     \\
f(\mA+\mB)&\le f(\mA)+f(\mB)
\end{align}
Many popular matrix norms also satisfy ``sub-multiplicativity": $f(\mA\mB)\le f(\mA)f(\mB)$.

\subsection{Frobenius norm}
\begin{align}
\norm{\mA}_F &= \sqrt{\trace\mA\mA^H}                           \\
             &= \sqrt{\sum_{i=1}^m \sum_{j=1}^n |\mA_{ij}|^2 }  \\
             &= \sqrt{\sum_{i=1}^m \eig(A^H A)_i }
\end{align}

\subsubsection{Special Properties}
\begin{equation}
\norm{\mA\vx}_2 \le \norm{\mA}_F \norm{\vx}_2~~~\vx\in\sRn
\end{equation}

\begin{equation}
\norm{\mA\mB}_F\le \norm{\mA}_F \norm{\mB}_F
\end{equation}

\subsection{Operator Norms}
For $p=1,2,\infty$ or other values, an operator norm indicates the maximum input-output gain of the matrix.
\begin{equation}
\norm{\mA}_p=\max_{\norm{\vu}_p=1} \norm{\mA\vu}_p
\end{equation}

\begin{align}
\norm{\mA}_1
  &=\max_{\norm{\vu}_1=1} \norm{\mA\vu}_1       \\
  &=\max_{j=1,\ldots,n} \sum_{i=1}^m |\mA_{ij}| \\
  &=\textrm{Largest absolute column sum}
\end{align}

\begin{align}
\norm{\mA}_\infty
  &=\max_{\norm{\vu}_\infty=1} \norm{\mA\vu}_\infty  \\
  &=\max_{j=1,\ldots,m} \sum_{i=1}^n |\mA_{ij}| \\
  &=\textrm{Largest absolute row sum}
\end{align}

\begin{align}
\norm{\mA}_2
  &=\textrm{``spectral norm"}                   \\
  &=\max_{\norm{\vu}_2=1} \norm{\mA\vu}_2       \\
  &=\sqrt{\max(\eig(\mA^T\mA))} \\
  &=\textrm{Square root of largest eigenvalue of~}\mA^T\mA
\end{align}



\subsubsection{Special Properties}
\begin{align}
\norm{\mA\vu}_p &\le \norm{\mA}_p \norm{\vu}_p \\
\norm{\mA\mB}_p &\le \norm{\mA}_p \norm{\mB}_p \\
\end{align}

\subsection{Spectral Radius}
Not a proper norm.
\begin{equation}
\rho(\mA)=\textrm{spectral radius}(\mA)=\max_{i=1,\ldots,n} | \eig(\mA)_i |
\end{equation}

\subsubsection{Special Properties}
\begin{align}
\rho(\mA) &\le \norm{\mA}_p \\
\rho(\mA) &\le \min(~\norm{\mA}_1, \norm{\mA}_\infty) \\
\end{align}


\section{Vectors}

P-norm:
\begin{equation}
\norm{\vx}_p = (\sum_i |\vx_i|^p)^{1/p}
\end{equation}

\chapter{Bounds}

\section{Matrix Gain}
\begin{equation}
\lambda_\textrm{min}(\mA^T\mA)\le \frac{\norm{\mA\vx}_2^2}{\norm{\vx}_2^2}\le\lambda_\textrm{max}(\mA^T\mA)
\end{equation}

\begin{equation}
\max_{\vx\ne0} \frac{\norm{\mA\vx}_2}{\norm{\vx}_2}=\norm{\mA}_2=\sqrt{\lambda_\textrm{max}(\mA^T\mA)}\implies\vx=u_1
\end{equation}

\begin{equation}
\min_{\vx\ne0} \frac{\norm{\mA\vx}_2}{\norm{\vx}_2}=\sqrt{\lambda_\textrm{min}(\mA^T\mA)}\implies\vx=u_n
\end{equation}


\section{Norms}

For $\vx\in\mathbb{R}^n$
\begin{equation}
\frac{1}{\sqrt{n}}\norm{\vx}_2
\le\norm{\vx}_\infty
\le\norm{\vx}_2
\le\norm{\vx}_1
\le\sqrt{\textrm{card}(\vx)}\norm{\vx}_2
\le\sqrt{n}\norm{\vx}_2
\le n \norm{\vx}_\infty
\end{equation}

For any $0<p<q$ we have that $\norm{\vx}_q\le\norm{\vx}_p$.

\section{Rayleigh quotients}
The Rayleigh quotient of $\mA\in\sSn$ is given by
\begin{equation}
\frac{\vx^T \mA \vx}{\vx^T\vx}~~\vx\ne0
\end{equation}

\begin{equation}
\lambda_\textrm{min}(\mA)\le \frac{\vx^T \mA \vx}{\vx^T\vx} \le \lambda_\textrm{max}(\mA)~~\vx\ne0
\end{equation}

\begin{align}
\lambda_\textrm{max}(A)&=\max_{\vx: \norm{\vx}_2=1} \vx^T\mA\vx=u_1 \\
\lambda_\textrm{min}(A)&=\min_{\vx: \norm{\vx}_2=1} \vx^T\mA\vx=u_n
\end{align}
where $u_1$ and $u_n$ are the eigenvectors associated with $\lambda_\textrm{max}$ and $\lambda_\textrm{min}$, respectively.







\chapter{Linear Equations}
The linear equation $\mA\vx=\vy$ with $\mA\in\sRmn$ admits a solution iff $\rank([\mA \vy])=\rank(\mA)$. If this is satisfied, the set of all solutions is an affine set $\mathcal{S}=\{\vx=\bar \vx+z: z\in\ns(\mA)\}$ where $\bar \vx$ is any vector such that $\mA\bar\vx=\vy$. The solution is unique if $\ns(\mA)=\{0\}$.

$\mA\vx=\vy$ is \textit{overdetermined} if it is tall/skinny ($m>n$); that is, if there are more equations than unknowns. If $\rank(\mA)=n$ then $\dim\ns(\mA)=0$, so there is either no solution or one solution. Overdetermined systems often have no solution ($\vy\notin\range(\mA)$), so an approximate solution is necessary.

$\mA\vx=\vy$ is \textit{underdetermined} if it is short/wide ($n>m$); that is, if has more unknowns than equations. If $\rank(\mA)=m$ then $\range(\mA)=\sRm$, so $\dim\ns(\mA)=n-m>0$, so the set of solutions is infinite. Therefore, finding a single solution that optimizes some quantity is of interest.

$\mA\vx=\vy$ is \textit{square} if $n=m$. If $\mA$ is invertible, then the equations have the unique solution $\vx=\mA^{-1}\vy$.

\section{Least-Squares}
For an overdetermined system we wish to find:
\begin{equation}
\min_\vx \norm{\mA\vx-\vy}_2^2
\end{equation}
Since $\mA\vx\in\range(\mA)$, we need a point $\tilde \vy = \mA\vx^*\in\range(\mA)$ closest to $\vy$. This point lies in the nullspace of $\mA^T$, so we have $\mA^T(\vy-\mA\vx^*)=0$. There is always a solution to this problem and, if $\rank(\mA)=n$, it is unique.
\begin{equation}
\vx^*=(\mA^T\mA)^{-1}\mA^T\vy
\end{equation} %TODO: Check

\section{Minimum Norm Solutions}
For undertermined systems in which $\mA\in\sRmn$ with $m<n$. We wish to find
\begin{equation}
\min_{\vx: \mA\vx=\vy} \norm{\vx}_2
\end{equation}
The solution $\vx^*$ must be orthogonal to $\ns(\mA)$, so $\vx^*\in\range(\mA^T)$, so $\vx^*=\mA^Tc$ for some $c$, so $\mA \mA^T c=\vy$, therefore:
\begin{equation}
\vx^*=\mA^T(\mA\mA^T)^{-1}\vy
\end{equation}















\chapter{$\mathbf{1}_r^T \mA \mathbf{1}_c$}

\textbf{Reduces to:} Scalar

\textbf{Notation:} For $\mA \in \mathbb{R}^{r\times c}$, $\mathbf{1}_r$ is in $\mathbb{R}^{r \times 1}$ and $\mathbf{1}_c$ is in $\mathbb{R}^{c \times 1}$.

\textbf{Plain English:} The sum of the elements of the matrix.

\textbf{Algorithm:} Traverse all the element of the matrix in order keeping track of the sum. For applications where accuracy is important and the matrices have a large dynamic range, Kahan summation or a similar technique should be used.

\textbf{Update Algorithm:} If an entry changes, subtract its old value from the sum and add its new value to the sum.



\chapter{$\vx^T \mA \vx$}

\textbf{Reduces to}: Scalar

\textbf{Notation:} $\mA$ must be in $\mathbb{R}^{i\times i}$. $\vx$ is in $\mathbb{R}^{i \times 1}$.

\textbf{Plain English:} TODO

\textbf{Algorithm:} TODO

\textbf{Update Algorithm:} We make use of the identity $(\vx^T \mA \vx)=\sum_{i,j}\big((\vx \vx^T)\circ\mA\big)$. If an entry $\mA_{i,j}$ in the matrix changes subtract its old value $\vx_i \vx_j \mA_{ij}$ and add the new value $\vx_i \vx_j \mA_{ij}'$. If an entry $\vx_i$ changes TODO.





\chapter{Algorithms}

\section{Gram-Schmidt}
TODO
% Consider the [[Gram–Schmidt process]] applied to the columns of the full column rank matrix <math>A=[\mathbf{a}_1, \cdots, \mathbf{a}_n]</math>, with [[inner product]] <math>\langle\mathbf{v},\mathbf{w}\rangle = \mathbf{v}^\top \mathbf{w}</math> (or <math>\langle\mathbf{v},\mathbf{w}\rangle = \mathbf{v}^* \mathbf{w}</math> for the complex case).

% Define the [[Vector projection|projection]]:
% :<math>\mathrm{proj}_{\mathbf{u}}\mathbf{a}
% = \frac{\left\langle\mathbf{u},\mathbf{a}\right\rangle}{\left\langle\mathbf{u},\mathbf{u}\right\rangle}{\mathbf{u}}
% </math>
% then:
% :<math>
% \begin{align}
%  \mathbf{u}_1 &= \mathbf{a}_1,
%   & \mathbf{e}_1 &= {\mathbf{u}_1 \over \|\mathbf{u}_1\|} \\
%  \mathbf{u}_2 &= \mathbf{a}_2-\mathrm{proj}_{\mathbf{u}_1}\,\mathbf{a}_2,
%   & \mathbf{e}_2 &= {\mathbf{u}_2 \over \|\mathbf{u}_2\|} \\
%  \mathbf{u}_3 &= \mathbf{a}_3-\mathrm{proj}_{\mathbf{u}_1}\,\mathbf{a}_3-\mathrm{proj}_{\mathbf{u}_2}\,\mathbf{a}_3,
%   & \mathbf{e}_3 &= {\mathbf{u}_3 \over \|\mathbf{u}_3\|} \\
%  & \vdots &&\vdots \\
%  \mathbf{u}_k &= \mathbf{a}_k-\sum_{j=1}^{k-1}\mathrm{proj}_{\mathbf{u}_j}\,\mathbf{a}_k,
%   &\mathbf{e}_k &= {\mathbf{u}_k\over\|\mathbf{u}_k\|}
% \end{align}
% </math>

% We can now express the <math>\mathbf{a}_i</math>s over our newly computed orthonormal basis:

% :<math>
% \begin{align}
%  \mathbf{a}_1 &= \langle\mathbf{e}_1,\mathbf{a}_1 \rangle \mathbf{e}_1  \\
%  \mathbf{a}_2 &= \langle\mathbf{e}_1,\mathbf{a}_2 \rangle \mathbf{e}_1
%   + \langle\mathbf{e}_2,\mathbf{a}_2 \rangle \mathbf{e}_2 \\
%  \mathbf{a}_3 &= \langle\mathbf{e}_1,\mathbf{a}_3 \rangle \mathbf{e}_1
%   + \langle\mathbf{e}_2,\mathbf{a}_3 \rangle \mathbf{e}_2
%   + \langle\mathbf{e}_3,\mathbf{a}_3 \rangle \mathbf{e}_3 \\
%  &\vdots \\
%  \mathbf{a}_k &= \sum_{j=1}^{k} \langle \mathbf{e}_j, \mathbf{a}_k \rangle \mathbf{e}_j
% \end{align}
% </math>
% where <math>\langle\mathbf{e}_i,\mathbf{a}_i \rangle = \|\mathbf{u}_i\|</math>. This can be written in matrix form:
% :<math> A = Q R </math>
% where:
% :<math>Q = \left[ \mathbf{e}_1, \cdots, \mathbf{e}_n\right] </math>
% and
% :<math>
% R = \begin{pmatrix}
% \langle\mathbf{e}_1,\mathbf{a}_1\rangle & \langle\mathbf{e}_1,\mathbf{a}_2\rangle &  \langle\mathbf{e}_1,\mathbf{a}_3\rangle  & \ldots \\
% 0                & \langle\mathbf{e}_2,\mathbf{a}_2\rangle                        &  \langle\mathbf{e}_2,\mathbf{a}_3\rangle  & \ldots \\
% 0                & 0                                       & \langle\mathbf{e}_3,\mathbf{a}_3\rangle                          & \ldots \\
% \vdots           & \vdots                                  & \vdots                                    & \ddots \end{pmatrix}.</math>















\bibliography{refs}


\end{document}

TODO:
Orthogonal matrix = all eigenvalues are 1 or -1

Centering matrix
Distance matrix



For two vectors $b$ and $x$, $p=\frac{b^Tx}{b^Tb}b$ is the projection of $x$ onto $b$.

TODO: Gilber Strang (2016, p563: Matrix Factorizations)

TODO: Strang 2016, p. 583, List of symbols and computer codes


TODO: Add Gram-Schmidt procedure
TODO: Add computational efficiency notes for QR


Highlighting matrix example: http://www.texample.net/tikz/examples/highlighting-matrix/