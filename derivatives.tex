%TODO: Include handy diagram from https://explained.ai/matrix-calculus/index.html

\chapter{Derivatives}

\section{Useful Rules for Derivatives}
For general $\mA$ and $\mX$ (no special structure):
\begin{align}
\partial\mA           &= 0~~\textrm{where $\mA$ is a constant} \\
\partial(c\mX)        &= c\partial\mX                          \\
\partial(\mX+\mY)     &= \partial\mX+\partial\mY               \\
\partial(\trace(\mX)) &= \trace(\partial(\mX))                 \\
\partial(\mX\mY)      &= (\partial\mX)\mY + \mX(\partial\mY)   \\
\partial(\mX\circ\mY) &= (\partial\mX)\circ\mY + \mX\circ(\partial\mY) \\
%TODO Kronecker x in circle equation 39 Matrix Cookbook
\partial(\mX^{-1})    &= -\mX^{-1}(\partial\mX)\mX^{-1}        \\
\partial(\det(\mX))   &= \trace(\textrm{adj}(\mX)\partial\mX)  \\
\partial(\det(\mX))   &= \det(\mX)\trace(\mX^{-1}\partial\mX)  \\
\partial(\ln(\det(\mX))) &= \trace(\mX^{-1}\partial\mX)        \\
\partial(\mX^T)       &= (\partial\mX)^T                       \\
\partial(\mX^H)       &= (\partial\mX)^H
\end{align}

\section{Gradient Notation}
For a matrix $\mA\in\sRnm$, the gradient is defined as:
\begin{equation}
\grad_\mA f(\mA)=
\begin{bmatrix}
\pd{f(\mA)}{\mA_{11}} & \pd{f(\mA)}{\mA_{12}} & \ldots & \pd{f(\mA)}{\mA_{1n}} \\
\pd{f(\mA)}{\mA_{21}} & \pd{f(\mA)}{\mA_{22}} & \ldots & \pd{f(\mA)}{\mA_{2n}} \\
\vdots                & \vdots                & \ddots & \vdots                \\
\pd{f(\mA)}{\mA_{m1}} & \pd{f(\mA)}{\mA_{m2}} & \ldots & \pd{f(\mA)}{\mA_{mn}}
\end{bmatrix}
\end{equation}
i.e.
\begin{equation}
(\grad_\mA f(\mA))_{ij}=\pd{f(\mA)}{\mA_{ij}}
\end{equation}
Note that the size of the gradient is always the same size as the entity to which it is taken. Also note that the gradient of a function is only defined if the function is real-valued, that is, if it returns a scalar value.

\section{Derivatives of Matrices and Vectors}

\subsection{First-Order}

In the following, $\mJ$ is the Single-Entry Matrix (\autoref{sec:rogue_single_entry}).
\begin{align}
\pd{\vx^T \va}{\vx}     &= \pd{\va^T \vx}{\vx} = \va          \\
\pd{\va^T\mX\vb}{\mX}   &= \va\vb^T                           \\
\pd{\va^T\mX^T\vb}{\mX} &= \vb\va^T                           \\
\pd{\va^T\mX\va}{\mX}   &= \pd{\va^T\mX^T\va}{\mX} = \va\va^T \\
\pd{\mX}{\mX_{ij}}      &= \mJ^{ij}                              %TODO: What is this? (MCB 73)
%TODO: MCB 74, 75
\end{align}

\section{Derivatives of vector norms}

\begin{align}
\pd{}{\vx}\norm{\vx-\va}_2 &= \frac{\vx-\va}{\norm{\vx-\va}_2} \\
\pd{}{\vx}\frac{\vx-\va}{\norm{\vx-\va}_2} &= \frac{\mI}{\norm{\vx-\va}_2}-\frac{(\vx-\va)(\vx-\va)^T}{\norm{\vx-\va}_2^3} \\
\pd{\norm{\vx}_2^2}{\vx} &= \pd{\norm{\vx^T\vx}_2}{\vx} = 2\vx
\end{align}

\section{Scalar by Vector}
\begin{center}
\begin{tabular}{l|Sc|Sc|Sc}
Qualifier                  & Expression                                    & Numerator layout                                   & Denominator layout                            \\
                           & $\pd{a}{x}$                                   & $\vzero^T$                                         & $\vzero$                                      \\
                           & $\pd{au(\vx)}{\vx}$                           & $a\pd{u}{\vx}$                                     & Same                                          \\
                           & $\pd{u(\vx)+v(\vx)}{\vx}$                     & $\pd{u}{\vx} + \pd{v}{\vx}$                        & Same                                          \\
                           & $\pd{u(\vx)v(\vx)}{\vx}$                      & $u\pd{v}{\vx} + v\pd{u}{\vx}$                      & Same                                          \\
                           & $\pd{g(u(\vx))}{\vx}$                         & $\pd{g(u)}{u}\pd{u}{\vx}$                          & Same                                          \\
                           & $\pd{f(g(u(\vx)))}{\vx}$                      & $\pd{f(g)}{g}\pd{g(u)}{u}\pd{u}{\vx}$              & Same                                          \\
                           & $\pd{\vu(\vx)^T\vv(\vx)}{\vx}$                & $\vu^T\pd{\vv}{\vx}+\vv^T\pd{\vu}{\vx}$            & $\pd{\vu}{\vx}\vv+\pd{\vv}{\vx}\vu$           \\
                           & $\pd{\vu(\vx)^T\mA\vv(\vx)}{\vx}$             & $\vu^T\mA\pd{\vv}{\vx}+\vv^T\mA^T\pd{\vu}{\vx}$    & $\pd{\vu}{\vx}\mA\vv+\pd{\vv}{\vx}\mA^T\vu$   \\
                           & $\md{f}{2}{\vx}{}{{\vx^T}}{}$                 &                                                    & $\mH$, the Hessian matrix                     \\
                           & $\pd{\va\cdot\vx}{\vx}=\pd{\vx\cdot\va}{\vx}$ & $\va^T$                                            & $\va$                                         \\
                           & $\pd{\vb^T\mA\vx}{\vx}$                       & $\vb^T\mA$                                         & $\mA^T\vb$                                    \\
                           & $\pd{\vx^T\mA\vx}{\vx}$                       & $\vx^T(\mA+\mA^T)$                                 & $(\mA+\mA^T)\vx$                              \\
$\mA$ symmetric            & $\pd{\vx^T\mA\vx}{\vx}$                       & $2\vx^T\mA$                                        & $2\mA\vx$                                     \\
                           & $\pd{\vx^T\mA\vx}{\vx}$                       & $\mA+\mA^T$                                        & Same                                          \\
$\mA$ symmetric            & $\pd{\vx^T\mA\vx}{\vx}$                       & $\mA$                                              & Same                                          \\
                           & $\pd{\vx^T\vx}{\vx}$                          & $2\vx^T$                                           & $2\vx$                                        \\
                           & $\pd{\va^T\vu(\vx)}{\vx}$                     & $\va^T\pd{\vu}{\vx}$                               & $\pd{\vu}{\vx}\va$                            \\
                           & $\pd{\va^T\vx\vx^T\vb}{\vx}$                  & $\vx^T(\va\vb^T+\vb\va^T)$                         & $(\va\vb^T+\vb\va^T)\vx$                      \\
                           & $\pd{(\mA\vx+\vb)^T\mC(\mD\vx+\ve)}{\vx}$     & $(\mD\vx+\ve)^T\mC^T\mA+(\mA\vx+\vb)^T\mC\mD$      & $\mD^T\mC^T(\mA\vx+\vb)+\mA^T\mC(\mD\vx+\ve)$ \\
                           & $\pd{\norm{\vx-\va}}{\vx}$                    & $\frac{(\vx-\va)^T}{\norm{\vx-\va}}$               & $\frac{\vx-\va}{\norm{\vx-\va}}$              \\                           
\end{tabular}
\end{center}

\section{Vector by Vector}
\begin{center}
\begin{tabular}{l|Sc|Sc|Sc}
Qualifier                  & Expression                                    & Numerator layout                                   & Denominator layout                            \\
                           & $\pd{\va}{\vx}$                               & $\vzero$                                           & Same                                          \\ %TODO: really the same? shouldn't be transposed?
                           & $\pd{\vx}{\vx}$                               & $\mI$                                              & Same                                          \\ %TODO: Really the identity matrix? Which one?
                           & $\pd{\mA\vx}{\vx}$                            & $\mA$                                              & $\mA^T$                                       \\
                           & $\pd{\vx^T\mA}{\vx}$                          & $\mA^T$                                            & $\mA$                                         \\
                           & $\pd{a\vu(\vx)}{\vx}$                         & $a\pd{\vu}{\vx}$                                   & Same                                          \\
                           & $\pd{a(\vx)\vu(\vx)}{\vx}$                    & $a\pd{\vu}{\vx}+\vu\pd{a}{\vx}$                    & $a\pd{\vu}{\vx}+\pd{a}{\vx}\vu^T$             \\
                           & $\pd{\mA\vu(\vx)}{\vx}$                       & $\mA\pd{\vu}{\vx}$                                 & $\pd{\vu}{\vx}\mA^T$                          \\
                           & $\pd{(\vu(\vx)+\vv(\vx))}{\vx}$               & $\pd{\vu}{\vx}+\pd{\vv}{\vx}$                      & Same                                          \\
                           & $\pd{\vg(\vu(\vx))}{\vx}$                     & $\pd{\vg(\vu)}{\vu}\pd{\vu}{\vx}$                  & $\pd{\vu}{\vx}\pd{\vg(\vu)}{\vu}$             \\
                           & $\pd{\vf(\vg(\vu(\vx)))}{\vx}$                & $\pd{\vf(\vg)}{\vg(\vu)}\pd{\vg(\vu)}{\vu}\pd{\vu}{\vx}$& $\pd{\vu}{\vx}\pd{\vg(\vu)}{\vu}\pd{\vf(\vg)}{\vg}$
\end{tabular}
\end{center}



\section{Matrix by Scalar}
\begin{center}
\begin{tabular}{l|Sc|Sc}
Qualifier                  & Expression                                     & Numerator layout                                       \\
                           & $\pd{a\mU(x)}{x}$                              & $a\pd{\mU}{x}$                                         \\
                           & $\pd{\mA\mU(x)\mB}{x}$                         & $\mA\pd{\mU}{x}\mB$                                    \\
                           & $\pd{(\mU(x)+\mV(x))}{x}$                      & $\pd{\mU}{x}+\pd{\mV}{x}$                              \\
                           & $\pd{(\mU(x)\mV(x))}{x}$                       & $\mU\pd{\mV}{x}+\pd{\mU}{x}\mV$                        \\
                           & $\pd{(\mU(x)\kp\mV(x))}{x}$                    & $\mU\kp\pd{\mV}{x} + \pd{\mU}{x}\kp\mV$                \\
                           & $\pd{(\mU(x)\hp\mV(x))}{x}$                    & $\mU\hp\pd{\mV}{x} + \pd{\mU}{x}\hp\mV$                \\
                           & $\pd{\mU^{-1}(x)}{x}$                          & $-\mU^{-1} \pd{\mU}{x} \mU^{-1}$                       \\
                           & $\md{\mU^{-1}}{2}{x}{}{y}{}$                   & $\mU^{-1}\left(\pd{\mU}{x}\mU^{-1}\pd{\mU}{y} - \md{\mU}{2}{x}{}{y}{} + \pd{\mU}{y}\mU^{-1}\pd{\mU}{x}\right)\mU^{-1}$ \\
                           & $\pd{e^{x\mA}}{x}$                             & $\mA e^{x\mA}=e^{x\mA}\mA$
\end{tabular}
\end{center}

\section{Traces}
\begin{align}
\pd{}{\mX}\trace(\mX)             &=\mI            \\
\pd{}{\mX}\trace(\mX\mA)          &=\mA^T          \\
\pd{}{\mX}\trace(\mA\mX\mB)       &=\mA^T\mB^T     \\
\pd{}{\mX}\trace(\mA\mX^T\mB)     &=\mB\mA         \\
\pd{}{\mX}\trace(\mX^T\mA)        &=\mA            \\
\pd{}{\mX}\trace(\mA\mX^T)        &=\mA            \\
\pd{}{\mX}\trace(\mA\kp\mX)       &=\trace(\mA)\mI 
\end{align}
For traces with many instances of $\mX$ we can apply an analogue of the product rule. For example:
\begin{equation}
\pd{}{\mX}\trace(\mA\mX\mB\mX\mC^T)=\pd{}{\mX}\trace(\mA\mX\mD)+\pd{}{\mX}\trace(\mE\mX\mC^T)=\mA^T\mD^T+\mE^T\mC
\end{equation}
where $\mD=\mB\mX\mC^T$ and $\mE=\mA\mX\mB$.